{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \"\"\"You are a simple yet powerful LLM AI language model with natural language processing capabilities designed to respond with concise and informative, summarization of all the context that was provided below from a given text. Your task is to analyze correctly and create a memorable guiding summary.\n",
    "\n",
    "You will use the conversation history provided after these instructions to answer questions or execute tasks to do your best to serve the instruction you receive at the end.\n",
    "\n",
    "You will never make up anything that was not present in or not related to the provided context. \n",
    "\n",
    "Your objective is to summarize the context below and present it in an understandable and compact fashion.\n",
    "\n",
    "You should be aware that this conversation is between other individuals and you are only observing without any contribution.\n",
    "\n",
    "Don't include any comments or speech in your response, output pure summarization and nothing else.\n",
    "\n",
    "Good luck and have fun!\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-15 22:58:22.377145: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-02-15 22:58:22.380407: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-02-15 22:58:22.389973: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1739649502.406526   18056 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1739649502.411495   18056 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-02-15 22:58:22.427996: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# 1. **Data Preparation**\n",
    "# Firstly, you need to prepare your dataset. You can use any text corpus for this purpose. For simplicity, let's assume we have a file named `data.txt` containing our training data.\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy\n",
    "\n",
    "# Split each line into a list\n",
    "lines = data.split(\"\\n\\n\")\n",
    "\n",
    "# Split each line into words\n",
    "words = \" \".join(lines).split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. **Tokenization**\n",
    "# Next, we need to convert our text data into numerical tensors that can be fed into a neural network.\n",
    "\n",
    "# Create a tokenizer\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(char_level=True)\n",
    "\n",
    "# Fit the tokenizer on the words\n",
    "tokenizer.fit_on_texts(words)\n",
    "\n",
    "# Convert the words back into sequences of integers\n",
    "sequences = tokenizer.texts_to_sequences(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1739649504.015847   18056 gpu_device.cc:2344] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "# 3. **Building the Model**\n",
    "# Now, we can build our language model using a recurrent neural network (RNN) like LSTM or GRU.\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "# Define the model architecture\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, 64),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
    "    tf.keras.layers.Dense(vocab_size)\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (150,) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# 4. **Training the Model**\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# We can now train our language model using the prepared data.\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Convert sequences into numpy arrays\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m sequences \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconstant(\u001b[43mnumpy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpand_dims\u001b[49m\u001b[43m(\u001b[49m\u001b[43msequences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Pad the sequences to make them of equal length\u001b[39;00m\n\u001b[1;32m      8\u001b[0m padded_sequences \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mpreprocessing\u001b[38;5;241m.\u001b[39msequence\u001b[38;5;241m.\u001b[39mpad_sequences(sequences, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/Projects/text_generation/lib/python3.11/site-packages/numpy/lib/_shape_base_impl.py:588\u001b[0m, in \u001b[0;36mexpand_dims\u001b[0;34m(a, axis)\u001b[0m\n\u001b[1;32m    586\u001b[0m     a \u001b[38;5;241m=\u001b[39m asarray(a)\n\u001b[1;32m    587\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 588\u001b[0m     a \u001b[38;5;241m=\u001b[39m \u001b[43masanyarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    590\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(axis) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m    591\u001b[0m     axis \u001b[38;5;241m=\u001b[39m (axis,)\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (150,) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "# 4. **Training the Model**\n",
    "# We can now train our language model using the prepared data.\n",
    "\n",
    "# Convert sequences into numpy arrays\n",
    "sequences = tf.constant(numpy.expand_dims(sequences, -1))\n",
    "\n",
    "# Pad the sequences to make them of equal length\n",
    "padded_sequences = tf.keras.preprocessing.sequence.pad_sequences(sequences, padding='post')\n",
    "\n",
    "# Filter out empty sequences\n",
    "padded_sequences = [seq for seq in padded_sequences if len(seq) > 0]\n",
    "\n",
    "# Train the model\n",
    "model.fit(padded_sequences, epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. **Evaluating the Model**\n",
    "# Finally, we evaluate the performance of our trained model.\n",
    "# Generate some text from the model\n",
    "generated_text = ''\n",
    "\n",
    "for i in range(20):\n",
    "    prediction = model.predict(tf.expand_dims([tokenizer.texts_to_sequences([generated_text])[0]], 0))\n",
    "    predicted_id = tf.argmax(prediction[0]).numpy()\n",
    "    generated_text += tokenizer.index_word[predicted_id]\n",
    "\n",
    "print(generated_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "text_generation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
